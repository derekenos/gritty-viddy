<html lang="en">

  <head>
    <meta charset="utf-8">

    <style>
     video,
     canvas {
       width: 1280;
       height: 720;
       border: solid #000 1px;
       display: none;
     }

     #final-wrapper {
       display: inline-block;
       position: relative;
     }

     #final-controls {
       position: absolute;
       top: 16px;
       right: 16px;
       background-color: rgba(0, 0, 0, .5);
       color: #fff;
       font-size: 16px;
       cursor: pointer;
     }

     canvas#final {
       display: block;
     }
    </style>

    <!-- Include gpu.js -->
    <script src="ext/gpu-browser.min.js"></script>

    <script type="module">
     import CanvasRecorder from "./CanvasRecorder.js"
     import * as CPU_FILTERS from "./filters.js"
     import * as GPU_FILTERS from "./gpuFilters.js"


     ///////////////////////////////////////////////////////////////////////////////
     // Constants
     ///////////////////////////////////////////////////////////////////////////////

     const USE_GPU = true
     const FILTERS = USE_GPU ? GPU_FILTERS : CPU_FILTERS

     const SOURCE_WIDTH = 1280
     const SOURCE_HEIGHT = 720

     const FINAL_SCALE = USE_GPU ? 1 : 1 / 4
     const FINAL_WIDTH = SOURCE_WIDTH * FINAL_SCALE
     const FINAL_HEIGHT = SOURCE_HEIGHT * FINAL_SCALE


     ///////////////////////////////////////////////////////////////////////////////
     // Utility Functions
     ///////////////////////////////////////////////////////////////////////////////

     async function initInputStreams () {
       const video = document.querySelector("video")
       video.srcObject = await navigator.mediaDevices
         .getUserMedia({
           audio: true,
           video: { width: SOURCE_WIDTH, height: SOURCE_HEIGHT },
         })

       // Mute the video audio and get its audio stream
       video.muted = true
       // from: https://stackoverflow.com/a/52400024/2327940
       const audioCtx = new AudioContext()
       const dest = audioCtx.createMediaStreamDestination()
       const sourceNode = audioCtx.createMediaElementSource(video)

       // Create an audioAnalyzer node so that we can use audio as an
       // input to the video filters.
       const audioAnalyser = audioCtx.createAnalyser()
       sourceNode.connect(audioAnalyser)
       audioAnalyser.connect(dest)

       // Uncomment to hear the audio in realtime.
       //sourceNode.connect(audioCtx.destination);
       const audioTrack = dest.stream.getAudioTracks()[0]

       return { video, audioTrack, audioAnalyser }
     }

     async function configureCanvi (audioTrack) {
       const workCanvas = document.querySelector("canvas#work")
       workCanvas.width = FINAL_WIDTH
       workCanvas.height = FINAL_HEIGHT
       const finalCanvas = document.querySelector("canvas#final")
       finalCanvas.width = FINAL_WIDTH
       finalCanvas.height = FINAL_HEIGHT

       // Add final canvas fullscreen button handler.
       document.getElementById("final-fullscreen")
         .addEventListener("click", () => {
           finalCanvas.requestFullscreen()
         })

       // Init the CanvasRecorder and configure the record button.
       const button = document.getElementById("final-record")
       button.textContent = "Start Recording"
       // Firefox complains that something isn't initialized if I try to
       // instantiate the CanvasRecorder here, so I deferred it.
       let recorder
       button.addEventListener("click", e => {
         if (recorder === undefined) {
           recorder = new CanvasRecorder(finalCanvas, audioTrack)
         }
         switch (recorder.state) {
           case "inactive":
             recorder.start()
             e.target.textContent = "Stop Recording"
             break
           case "recording":
             recorder.stop()
             e.target.textContent = "Start Recording"
             break
           default:
             throw new Error(`Unhandled recorder state: ${recorder.state}`)
             break
         }
       })

       return { workCanvas, finalCanvas }
     }

     function getAudioParams (audioAnalyser, audioBuffer) {
       // Get the audio sample data.
       audioAnalyser.getByteTimeDomainData(audioBuffer)
       // Calculate the average loudness as a float in the range 0 - 1
       const positiveAudioSamples = audioBuffer.filter(x => x > 128)
       const normalizedLoudness = !positiveAudioSamples.length
         ? 0
         : positiveAudioSamples.reduce((acc, x) => acc + (x - 128) / 128, 0) / positiveAudioSamples.length
       // Scale the audio samples to floats in the range 0 - 1 and
       const normalizedSamples = new Array(...audioBuffer).map(x => x / 255)
       let scaledNormalizedSamples = []
       const idxFactor = Math.floor(normalizedSamples.length / FINAL_WIDTH)
       for (let i = 0; i < FINAL_WIDTH; i += 1) {
         scaledNormalizedSamples.push(normalizedSamples[i * idxFactor])
       }
       return { normalizedLoudness, scaledNormalizedSamples }
     }

     async function init () {
       const { video, audioTrack, audioAnalyser } = await initInputStreams()
       // Create a buffer to use for sampling the audio.
       const audioBuffer = new Uint8Array(audioAnalyser.fftSize)

       const { workCanvas, finalCanvas } = await configureCanvi(audioTrack)
       const workCtx = workCanvas.getContext("2d")
       const finalCtx = finalCanvas.getContext("2d")

       function getProcessedImageData (filters = []) {
         // Capture a frame from the video and draw it to the work canvas.
         workCtx.drawImage(video, 0, 0, workCanvas.width, workCanvas.height)
         // Get the pixel data from the work canvas.
         let imageData = workCtx.getImageData(0, 0, workCanvas.width, workCanvas.height)
         // Apply the filters to the pixel data and return the result.
         return FILTERS.applyFilters(filters, imageData)
       }

       function process () {
         // Get audio data that we can use to modify the filter parameters.
         const {
           normalizedLoudness,
           scaledNormalizedSamples
         } = getAudioParams(audioAnalyser, audioBuffer)

         // Define the filter chain.
         const filters = [
           //[ FILTERS.thresholdFilter, [ 255 - normalizedLoudness * 4 * 255 ] ],
           //[ FILTERS.brightnessFilter, [ 0 + normalizedLoudness * 10] ],
           //[ FILTERS.channelFilter, [ 1, 1, 1 ] ],
           //[ FILTERS.colorReducerFilter, [ 0x80 ] ],
           //[ FILTERS.rowBlankerFilter, [ 2 ] ],
           //[ FILTERS.colBlankerFilter, [ 8 ] ],
           //[ FILTERS.colorGainFilter, [ 0, 0, normalizedLoudness * 10 ] ],
           //[ FILTERS.invertFilter, [] ]
           [ FILTERS.audioPlotFilter, scaledNormalizedSamples ],
         ]

         // Use the filters to process the image and write the result to
         // the final canvas.
         finalCtx.putImageData(getProcessedImageData(filters), 0, 0)

         // Repeat the process on the next animation frame.
         requestAnimationFrame(process)
       }

       // Get the process started.
       requestAnimationFrame(process)
     }

     document.addEventListener("DOMContentLoaded", init)
    </script>

  </head>

  <body>
    <video autoplay="true"></video>
    <canvas id="work"></canvas>
    <div id="final-wrapper">
      <div id="final-controls">
        <button id="final-record"></button>
        <button id="final-fullscreen">&#8644;</button>
      </div>
      <canvas id="final"></canvas>
    </div>
  </body>

</html>
