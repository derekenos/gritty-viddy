<html lang="en">

  <head>
    <meta charset="utf-8">

    <style>
     video,
     canvas {
       width: 1280px;
       height: 720px;
       border: solid #000 1px;
       display: none;
     }

     #final-wrapper {
       display: inline-block;
       position: relative;
     }

     #final-controls {
       position: absolute;
       top: 16px;
       right: 16px;
       background-color: rgba(0, 0, 0, .5);
       color: #fff;
       font-size: 16px;
       cursor: pointer;
     }

     canvas#final {
       display: block;
     }
    </style>

    <script src="ext/gpu-browser.min.js"></script>

    <script type="module">
     import Animator from "./animator.js"
     import CanvasRecorder from "./CanvasRecorder.js"
     import * as CPU_FILTERS from "./filters.js"
     import * as GPU_FILTERS from "./gpuFilters.js"

     const USE_GPU = true
     const FILTERS = USE_GPU ? GPU_FILTERS : CPU_FILTERS

     const dumpFrameToCanvas = (video, canvas) =>
       canvas.getContext("2d").drawImage(video, 0, 0, canvas.width, canvas.height)

     function initRecordControls (button, canvas, audioTrack) {
       button.textContent = "Start Recording"
       const recorder = new CanvasRecorder(canvas, audioTrack)
       button.addEventListener("click", e => {
         switch (recorder.state) {
           case "inactive":
             recorder.start()
             e.target.textContent = "Stop Recording"
             break
           case "recording":
             recorder.stop()
             e.target.textContent = "Start Recording"
             break
           default:
             throw new Error(`Unhandled recorder state: ${recorder.state}`)
             break
         }
       })
     }

     function init () {
       // Start video stream.
       const video = document.querySelector("video")
       const width = 1280
       const height = 720
       navigator.mediaDevices
         .getUserMedia({
           audio: true,
           video: { width, height },
         })
         .then(stream => {
           video.srcObject = stream
         })

       // Mute the video audio and get its audio stream
       video.muted = true
       // from: https://stackoverflow.com/a/52400024/2327940
       const audioCtx = new AudioContext()
       const dest = audioCtx.createMediaStreamDestination()
       const sourceNode = audioCtx.createMediaElementSource(video)

       // Create an audioAnalyzer node so that we can use audio as an
       // input to the video filters.
       const audioAnalyser = audioCtx.createAnalyser()
       sourceNode.connect(audioAnalyser)
       audioAnalyser.connect(dest)

       // Uncomment to hear the audio in realtime.
       //sourceNode.connect(audioCtx.destination);
       const audioTrack = dest.stream.getAudioTracks()[0]


       const canvasScale = USE_GPU ? 1 : 1 / 4
       const workCanvas = document.querySelector("canvas#work")
       workCanvas.width = width * canvasScale
       workCanvas.height = height * canvasScale
       const workCanvasCtx = workCanvas.getContext("2d")
       const finalCanvas = document.querySelector("canvas#final")
       finalCanvas.width = width * canvasScale
       finalCanvas.height = height * canvasScale
       const finalCanvasCtx = finalCanvas.getContext("2d")

       // Add fullscreen button handler.
       document.getElementById("final-fullscreen")
         .addEventListener("click", () => {
           finalCanvas.requestFullscreen()
         })

       function getProcessedImageData (filters = []) {
         dumpFrameToCanvas(video, workCanvas)
         const ctx = workCanvas.getContext("2d")
         let imageData = ctx.getImageData(0, 0, workCanvas.width, workCanvas.height)
         imageData = FILTERS.applyFilters(filters, imageData)
         return imageData
       }

       // Configure the things we'll use to control dynamic video effects.
       const anim = Animator({ stepsPerCycle: 50, oneShot: false })
       const audioBuffer = new Uint8Array(audioAnalyser.fftSize)

       function process () {
         // Calculate the audio loudness.
         audioAnalyser.getByteTimeDomainData(audioBuffer)
         const positiveAudioSamples = audioBuffer.filter(x => x > 128)
         const audioLevel = !positiveAudioSamples.length
           ? 0
           : positiveAudioSamples.reduce((acc, x) => acc + (x - 128) / 128, 0) / positiveAudioSamples.length

         const filters = [
           //[ FILTERS.thresholdFilter, [ Math.floor(anim.linearInterpolate( 128, 255 )) ] ],
           [ FILTERS.brightnessFilter, [ 1 + audioLevel * 5] ],
           //[ FILTERS.channelFilter, [ 1, 1, 1 ] ],
           //[ FILTERS.colorReducerFilter, [ 0xc0 ] ],
           //[ FILTERS.rowBlankerFilter, [ Math.floor(anim.linearInterpolate( 2, 32 )) ] ],
           //[ FILTERS.colBlankerFilter, [ 8 ] ],
           //[ FILTERS.colorGainFilter, [ -1, 0, 1 ] ],
         ]
         if (audioLevel > 0.2) {
           filters.push([ FILTERS.invertFilter, [] ])
         }
         finalCanvasCtx.putImageData(getProcessedImageData(filters), 0, 0)
         anim.step()
         requestAnimationFrame(process)
       }
       requestAnimationFrame(process)

       initRecordControls(
         document.getElementById("final-record"),
         document.querySelector("canvas#final"),
         audioTrack
       )
     }

     document.addEventListener("DOMContentLoaded", init)
    </script>

  </head>

  <body>
    <video autoplay="true"></video>
    <canvas id="work"></canvas>
    <div id="final-wrapper">
      <div id="final-controls">
        <button id="final-record"></button>
        <button id="final-fullscreen">&#8644;</button>
      </div>
      <canvas id="final"></canvas>
    </div>
  </body>

</html>
